---
title: Text to Speech
---

<Tabs>
  <Tab title="Voice Selection">
    <Frame caption="Voice Selection Dropdown">
        <img
            src="../../product/speech-synthesis/images/tts_voices.webp"
        />
    </Frame>

  </Tab>
  <Tab title="Model Selection">
    <Frame caption="Model Selection">
        <img
            src="../../product/speech-synthesis/images/tts_models.webp"
        />
    </Frame>

  </Tab>
  <Tab title="Voice Settings">
    <Frame caption="Voice Selection">
        <img
            src="../../product/speech-synthesis/images/tts_settings.webp"
        />
    </Frame>
    
  </Tab>
</Tabs>

Getting yourself familiar with these different settings and options will be very important in getting the best possible result. For Text to Speech, there are three main selections you need to make.

<AccordionGroup>
    <Accordion title="Voices">
        We offer many types of voices, including Default Voices that have been specfically curated to be the highest quality; completely synthetic voices created using our Voice Design tool; you can create your own collection of cloned voices using our two technologies: Instant Voice Clones and Professional Voice Clones; browse through our voice library to find the perfect voice for your production.

        Not all voices are equal, and a lot depends on the source audio used to create that voice. Some voices will perform better than others, while some will be more stable than others. Additionally, certain voices will be more easily cloned by the AI than others, and some voices may work better with one model and one language compared to another. All of these factors are important to consider when selecting your voice.

        <Card
        title="Read more..."
        icon="regular book"
        iconPosition="left"
        href="/docs/product/speech-synthesis/voice-selection"
        />

    </Accordion>

    <Accordion title="Models">
        As of December 2024, ElevenLabs offers two families of models: standard (high-quality) models and Flash models, which are optimized for low latency. Each family includes both English-only and multilingual models, tailored for specific use cases with strengths in either speed, accuracy, or language diversity.

        - **Standard models** (Multilingual v2, Multilingual v1, English v1) are optimized for quality and accuracy, ideal for content creation. These models offer the best quality and stability but have higher latency.
        - **Flash models** (Flash v2, Flash v2.5) are designed for low-latency applications like real-time conversational AI. They deliver great performance with faster processing speeds, though with a slight trade-off in accuracy and stability.

        If you want to find more detailed specifications about which languages each model offers, you can find all that information in our help article [here](https://help.elevenlabs.io/hc/en-us/articles/17883183930129-What-models-do-you-offer-and-what-is-the-difference-between-them-).

        For advice on how to deal with issues that might arise, please see our [guide to troubleshooting.]()

        <Card
        title="Read more..."
        icon="regular book"
        iconPosition="left"
        href="/docs/product/speech-synthesis/voice-selection"
        />

    </Accordion>

    <Accordion title="Settings">
        Our users have found different workflows that work for them. The one you'll see most often is setting stability around 50 and similarity near 75, with minimal changes thereafter. Of course, this all depends on the original voice and the style of performance you're aiming for.

        It's important to note that the AI is non-deterministic; setting the sliders to specific values won't guarantee the same results every time. Instead, the sliders function more as a range, determining how wide the randomization can be between each generation. Setting stability low means a wider range of randomization, often resulting in a more emotive performance, but this is also highly dependent on the voice itself.

        For a more lively and dramatic performance, it is recommended to set the stability slider lower and generate a few times until you find a performance you like.

        On the other hand, if you want a more serious performance, even bordering on monotone on very high values, it is recommended to set the stability slider higher. And since it's more consistent and stable, you usually don't need to do as many generations to get what you are looking for. Experiment to find what works best for you!

        <Card
        title="Read more..."
        icon="regular book"
        iconPosition="left"
        href="/docs/product/speech-synthesis/voice-selection"
        />

    </Accordion>

</AccordionGroup>

## Good to know

<AccordionGroup>
    <Accordion title="Good input equals good output">
        The first factor, and one of the most important, is that good, high-quality, and consistent input will result in good, high-quality, and consistent output.

        If you provide the AI with audio that is less than ideal—for example, audio with a lot of noise, reverb on clear speech, multiple speakers, or inconsistency in volume or performance and delivery—the AI will become more unstable, and the output will be more unpredictable.

        If you plan on cloning your own voice, we strongly recommend that you go through our guidelines in the documentation for creating proper voice clones, as this will provide you with the best possible foundation to start from. Even if you intend to use only Instant Voice Clones, it is advisable to read the Professional Voice Cloning section as well. This section contains valuable information about creating voice clones, even though the requirements for these two technologies are slightly different.

    </Accordion>

    <Accordion title="Use the right voice">
        The second factor to consider is that the voice you select will have a tremendous effect on the output. Not only, as mentioned in the first factor, is the quality and consistency of the samples used to create that specific clone extremely important, but also the language and tonality of the voice.

        If you want a voice that sounds happy and cheerful, you should use a voice that has been cloned using happy and cheerful samples. Conversely, if you desire a voice that sounds introspective and brooding, you should select a voice with those characteristics.

        However, it is also crucial to use a voice that has been trained in the correct language. For example, all of the professional voice clones we offer as default voices are English voices and have been trained on English samples. Therefore, if you have them speak other languages, their performance in those languages can be unpredictable. It is essential to use a voice that has been cloned from samples where the voice was speaking the language you want the AI to then speak.

    </Accordion>

    <Accordion title="Use proper formatting">
        This may seem slightly trivial, but it can make a big difference. The AI tries to understand how to read something based on the context of the text itself, which means not only the words used but also how they are put together, how punctuation is applied, the grammar, and the general formatting of the text.

        This can have a small but impactful influence on the AI's delivery. If you were to misspell a word, the AI won't correct it and will try to read it as written.

    </Accordion>

    <Accordion title="Nondeterministic">
        The settings of the AI are nondeterministic, meaning that even with the same initial conditions (voice, settings, model), it will give you slightly different output, similar to how a voice actor will deliver a slightly different performance each time.

        This variability can be due to various factors, such as the options mentioned earlier: voice, settings, model. Generally, the breadth of that variability can be controlled by the stability slider. A lower stability setting means a wider range of variability between generations, but it also introduces inter-generational variability, where the AI can be a bit more performative.

        A wider variability can often be desirable, as setting the stability too high can make certain voices sound monotone as it does give the AI the same leeway to generate more variable content. However, setting the stability too low can also introduce other issues where the generations become unstable, especially with certain voices that might have used less-than-ideal audio for the cloning process.

        The default setting of 50 is generally a great starting point for most applications.
    </Accordion>

</AccordionGroup>

We offer many types of voices, including Default Voices that have been specifically curated to be of the highest quality; completely synthetic voices created using our Voice Design tool; and the option to create your own collection of cloned voices using our two technologies: Instant Voice Clones and Professional Voice Clones. You can browse through our voice library to find the perfect voice for your production.

Not all voices are equal, and a lot depends on the source audio used to create that voice. Some voices will perform better than others, while some will be more stable than others. Additionally, certain voices will be more easily cloned by the AI than others, and some voices may work better with one model and one language compared to another. All of these factors are important to consider when selecting your voice.

<Frame caption="Voice Selection Dropdown">
  <img width="400" height="100%" src="../../product/speech-synthesis/images/tts_voices.webp" />
</Frame>

## Default Voices

Default voices are a curated set of voices optimized for core use cases and available to all ElevenLabs users by default. They are designed to ensure long-term availability, consistent quality, and priority support for new model developments.

These voices are crafted through multi-year partnerships with voice actors, making them reliable for extended use.

## Clone Voices

Cloned voices are created using either Instant Voice Cloning or Professional Voice Cloning.

- **Instant Voice Cloning** allows you to clone a voice using short audio samples, providing quick results but with less fidelity. This method is suitable for creating a basic clone without extensive training.

- **Professional Voice Cloning** involves training a model on larger datasets of a specific speaker's voice, resulting in a more accurate and realistic clone. This method is available for users on the Creator plan or higher and requires more time and resources.

Cloned voices are private and not shared publicly unless specifically whitelisted or shared through the Voice Library.

## Synthetic Voices

Synthetic voices are generated by AI using the Voice Design tool. They are created from text prompts and offer flexibility in attributes like gender, age, and accent. These voices are not based on real human voices and can be used to fill gaps when specific voices aren't available in the Voice Library. Synthetic voices cannot be shared with others and are available to all users for creating unique voice outputs.

## Voice Library

The Voice Library is a marketplace where the community can share their Professional Voice Clones for others to use. It offers a wide variety of voices contributed by users, allowing you to explore and utilize different voice options. You can search for voices using filters like language, accent, and more to find the ideal voice for your needs.

Our users have found different workflows that work for them. The one you'll see most often is setting stability around 50 and similarity near 75, with minimal changes thereafter. Of course, this all depends on the original voice and the style of performance you're aiming for.

It's important to note that the AI is non-deterministic; setting the sliders to specific values won't guarantee the same results every time. Instead, the sliders function more as a range, determining how wide the randomization can be between each generation. Setting stability low means a wider range of randomization, often resulting in a more emotive performance, but this is also highly dependent on the voice itself.

For a more lively and dramatic performance, it is recommended to set the stability slider lower and generate a few times until you find a performance you like.

On the other hand, if you want a more serious performance, even bordering on monotone at very high values, it is recommended to set the stability slider higher. Since it's more consistent and stable, you usually don't need to do as many generations to get what you are looking for. Experiment to find what works best for you!

<Frame caption="Voice Settings">
  <img width="400" height="100%" src="../../product/speech-synthesis/images/tts_settings.webp" />
</Frame>

## Stability

The stability slider determines how stable the voice is and the randomness between each generation. Lowering this slider introduces a broader emotional range for the voice. As mentioned before, this is also influenced heavily by the original voice. Setting the slider too low may result in odd performances that are overly random and cause the character to speak too quickly. On the other hand, setting it too high can lead to a monotonous voice with limited emotion.

## Similarity

The similarity slider dictates how closely the AI should adhere to the original voice when attempting to replicate it. If the original audio is of poor quality and the similarity slider is set too high, the AI may reproduce artifacts or background noise when trying to mimic the voice if those were present in the original recording.

## Style Exaggeration

With the introduction of the newer models, we also added a style exaggeration setting. This setting attempts to amplify the style of the original speaker. It does consume additional computational resources and might increase latency if set to anything other than 0. It's important to note that using this setting has shown to make the model slightly less stable, as it strives to emphasize and imitate the style of the original voice.

In general, we recommend keeping this setting at 0 at all times.

## Speaker Boost

This is another setting that was introduced in the new models. The setting itself is quite self-explanatory – it boosts the similarity to the original speaker. However, using this setting requires a slightly higher computational load, which in turn increases latency. The differences introduced by this setting are generally rather subtle.

## Cross link to models

---

title: 'Speech Synthesis: Generating Audio'
slug: product/guides/speech-synthesis

---

**Speech Synthesis** allows you to generate lifelike speech from text (**Text to Speech**) or audio (**Speech to Speech**) inputs. In this section, you can also see your generation history and thus retrieve past generations.

Selecting **Advanced Mode** allows you to select the model you would like to use for your generation as well as the voice settings (Stability, Similarity, Style, and Speaker Boost) on top of the existing options with Standard.

Let’s touch on models and voice settings briefly before generating our audio clip.

### Models

More detailed information about the models is available [here](/docs/product/speech-synthesis/models).

- **Multilingual v2 (default)**: Supports 28 languages, known for its accuracy and stability, especially when using high-quality samples.
- **Flash v2.5**: Generates speech in 32 languages with low latency, ideal for real-time applications.
- **Flash v2**: Optimized for low-latency English text-to-speech, similar in performance to Flash v2.5.
- **English v1**: The oldest and fastest model, best for audiobooks but less accurate.
- **Multilingual v1**: Experimental, surpassed by Multilingual v2, recommended for short text chunks.

### Voice Settings

More detailed information about the voice settings is available [here](/docs/product/speech-synthesis/voice-settings).

- **Stability**: Adjusts the emotional range and consistency of the voice. Lower settings result in more variation and emotion, while higher settings produce a more stable, monotone voice.
- **Similarity**: Controls how closely the AI matches the original voice. High settings may replicate artifacts from low-quality audio.
- **Style Exaggeration**: Enhances the speaker's style, but can affect stability.
- **Speaker Boost**: Increases the likeness to the original speaker, useful for weaker voices.

Now that we understand models and voice settings a bit better, let's jump into generating audio!

---

### Text to Speech (TTS)

**Step by step to Generate Text to Speech Audio**

1. **Text Input**: Type or paste your text into the input box on the Speech Synthesis page.
2. **Select Voice**: Select the voice you wish to use from your Voices at the bottom left of the screen.
3. **Adjust Settings**: Modify the voice settings for the desired output.

**Generate**: Click the 'Generate' button to create your audio file.

<Frame>
  <img src="../../product/guides/images/text-to-speech.webp" />
</Frame>

**Exercise**: Generate “All work and no play makes Jack a dull boy” using Alice’s voice (or a voice of your choosing).

---

### Speech to Speech (STS)

**Steps to Generate Speech to Speech Audio**

1. **Audio Input**: Upload or record audio via the input box on the Speech Synthesis page.
2. **Select Voice**: Select the voice you wish to use from your Voices at the bottom left of the screen.
3. **Adjust Settings**: Modify the voice settings for the desired output.

**Generate**: Click the 'Generate' button to create your audio file.

<Frame>
  <img src="../../product/guides/images/speech-to-speech.webp" />
</Frame>

<Tip>
  Speech to Speech is great for getting the right emotion across when the Text to Speech can’t get
  it right.
</Tip>

**Exercise**: Record yourself saying and generate “The cellar door is open, revealing a world of hidden treasures.” using Brian’s voice (or a voice of your choosing).
